{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5175ba-6c89-4fbe-a2fd-9372e5e57fde",
   "metadata": {},
   "source": [
    "# 1. Keras 텍스트 전처리\n",
    "    1. [수치 데이터로 변환] 텍스트 토큰화 및 정수 인코딩\n",
    "    2. [같은 길이로 만들기] 패딩 (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2566c6a4-7cd9-41fd-b4c6-d22cd8c2ca58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow 설치 확인\n",
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55fd7a2-947e-4448-97d4-7e6a32616d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습 샘플\n",
    "texts = [\n",
    "    \"나는 학교에 간다\",\n",
    "    \"너는 도서관에 간다\",\n",
    "    \"그는 시장에 간다\"\n",
    "]\n",
    "#texts = [ [\"나\", \"학교\", \"가\"], [\"너\", \"도서관\", \"가\"], [\"그\", \"시장\", \"가\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1eae6-33b8-41e0-9410-25e62e18555e",
   "metadata": {},
   "source": [
    "## 1-1. 텍스트 데이터 Integer Encoding\n",
    "- num_words = 사용할 단어 수 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2be7fb-81b9-4a83-a8cd-2fb3b7be91f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전 :  {'<OOV>': 1, '간다': 2, '나는': 3, '학교에': 4, '너는': 5, '도서관에': 6, '그는': 7, '시장에': 8}\n",
      "정수 인코딩 결과 :  [[3, 4, 2], [1, 1, 2], [1, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer 객체 생성 (특징 벡터용 단어 수와 OOV 토큰 지정)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = 5, oov_token = \"<OOV>\")\n",
    "\n",
    "# 특징 집합 생성 및 정수 indexing\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(\"단어 사전 : \", tokenizer.word_index)\n",
    "\n",
    "# 입력 텍스트 리스트의 integer encoding\n",
    "encoded_texts = tokenizer.texts_to_sequences(texts)\n",
    "print(\"정수 인코딩 결과 : \", encoded_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0c588-da3e-4603-a7e2-c91d9555bf4d",
   "metadata": {},
   "source": [
    "## 1-2. Padding : 입력 시퀀스의 길이를 동일하게 조정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9337d9-891d-4360-bff9-979721460717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩된 시퀀스 : \n",
      "[[0 0 3 4 2]\n",
      " [0 0 1 1 2]\n",
      " [0 0 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_sequences = pad_sequences(encoded_texts, maxlen=5, padding='post', truncating='post')\n",
    "print(\"패딩된 시퀀스 : \")\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea93547-cb44-4e79-bc8f-5795f9075515",
   "metadata": {},
   "source": [
    "## 1-3. 정답 데이터 One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae2d1a3-14e3-483e-b9e0-06905c7debe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = [1, 0, 2, 5, 0, 2, 3]\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoded_label = to_categorical(label)\n",
    "encoded_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1fd2c-8373-46b5-ab1b-9f2eb6593860",
   "metadata": {},
   "source": [
    "# 2. 텍스트 전처리를 위한 파라미터 결정하기\n",
    "- Integer Encoding을 위한 단어(특징) 집합의 크기 정하기 (Tokenizer의 num_words)\n",
    "- Padding을 위한 시퀀스 길이 정하기 (pad_sequences의 max_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ebd4eb7-9361-41ed-a8d5-fb0b930194d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['부산 행 때문 너무 기대하고 봤', '한국 좀비 영화 어색하지 않게 만들어졌 놀랍']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩 및 실습 데이터 추출\n",
    "import pandas as pd\n",
    "texts = list(pd.read_csv('./Korean_movie_reviews_2016.csv').review)\n",
    "texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f50e1-ce73-41a4-911e-79c26cf0a8e9",
   "metadata": {},
   "source": [
    "## 2-1. 단어 집합의 크기 정하기 (Tokenizer의 num_words)\n",
    "등장 빈도수가 기준값(threshold)보다 적은 단어의 비중을 확인하여 정하기\n",
    "1. corpus의 전체 단어 수 (total_cnt)와 전체 빈도수 (total_freq) 계산\n",
    "2. threshold보다 빈도수가 적은 단어수 카운트 (rare_cnt)와 빈도수 (rare_freq) 계산\n",
    "3. 희귀 단어의 비율(rare_cnt/total_cnt)과 희귀 등장 빈도 비율 계산(rare_freq/total_freq)\n",
    "4. 희귀 단어를 뺀 단어 수 -> 단어 집합의 크기 산정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca6156-3b72-4b32-b590-380ba57056b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer로 단어-빈도 사전 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8bda56-57fe-4455-9351-ff96c1f0811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도수가 threshold회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b194b0-6998-4c46-804e-eba9d12a0d13",
   "metadata": {},
   "source": [
    "## 2-2. Padding 길이 정하기 (pad_sequences의 max_len)\n",
    "1. 단어 길이 데이터로 DataFrame 생성\n",
    "2. 분포 시각화 : DataFrame의 Histogram 시각화 (df.hist())\n",
    "3. 단어 길이 통계 정보 확인 (df.describe())\n",
    "4. 텍스트의 길이가 설정한 max_len 이하인 비율 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e04d38-2e8d-4263-8a8b-439afb26622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 길이 데이터로 DataFrame 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3031e16-7a95-4185-9760-8fae64350608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 분포 시각화 : DataFrame의 Histogram 시각화 (df.hist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa57894-fac6-4170-91d0-15b21aad9f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#단어 길이 통계 정보 확인 (df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc375e9-83ee-4ac2-a960-8a99756bacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트의 길이가 설정한 max_len 이하인 비율 계산\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepNLPEnv",
   "language": "python",
   "name": "deepnlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
