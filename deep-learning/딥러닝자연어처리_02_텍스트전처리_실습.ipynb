{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d65176",
   "metadata": {},
   "source": [
    "# 텍스트 전처리, Text Preprocessing\n",
    "---\n",
    "1. **자연어처리 라이브러리 설치** : NLTK, KoNLPy\n",
    "1. **토근화 (Tokenization)** : 데이터를 의미있는 기본 단위로 분리하는 작업\n",
    "1. **품사 태깅 (POS Tagging)** : 토큰에 품사를 태깅하는 작업\n",
    "1. **정제 (Cleaning)** : 불필요한 기호나 문자를 제거하는 작업\n",
    "1. **정규화 (Normalization)** : 형태가 다른 단어를 하나의 형태로 통합하는 작업. 대/소문자 통합, 유사의미 단어통합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18259d",
   "metadata": {},
   "source": [
    "## 1. 자연어 처리 라이브러리 설치 (NLTK, KoNLPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb02f851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b424504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK data 설치\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6939682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f01255",
   "metadata": {},
   "source": [
    "## 2. 토큰화 (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683b04b",
   "metadata": {},
   "source": [
    "### 2-1. NLTK를 활용한 토큰화 : nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68de1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, everyone. It's good to see you. Let's start out NLP class!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "998ece79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 문장 토큰화\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# 단어 토큰화\n",
    "sent_tokens = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace1c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'everyone.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'start',\n",
       " 'out',\n",
       " 'NLP',\n",
       " 'class',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPunctTokenizer, TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer, TreebankWordTokenizer\n",
    "wpTokenizer = WordPunctTokenizer()\n",
    "wpTokenizer.tokenize(text)\n",
    "\n",
    "tbTokenizer = TreebankWordTokenizer()\n",
    "tbTokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb19a26",
   "metadata": {},
   "source": [
    "### 2-2. 한국어 토큰화 : konlpy.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4608f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = \"안녕하세요, 여러분. 만나서 반갑습니다. 지금부터 자연어처리 수업을 시작하겠습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4627ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '지금부터 자연어처리 수업을 시작하겠습니다.']\n",
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '지금부터', '자연어처리', '수업을', '시작하겠습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk로 토큰화\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 문장 분리\n",
    "print(sent_tokenize(kor_text))\n",
    "\n",
    "# 단어 토큰화\n",
    "print(word_tokenize(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69eab0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요',\n",
       " ',',\n",
       " '여러분',\n",
       " '.',\n",
       " '만나서',\n",
       " '반갑습니다',\n",
       " '.',\n",
       " '지금',\n",
       " '부터',\n",
       " '자연어',\n",
       " '처리',\n",
       " '수업',\n",
       " '을',\n",
       " '시작',\n",
       " '하겠습니다',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okt로 토큰화 \n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "okt.morphs(kor_text)    #형태소 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc6351e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요',\n",
       " ',',\n",
       " '여러분',\n",
       " '.',\n",
       " '만나',\n",
       " '아서',\n",
       " '반갑습니다',\n",
       " '.',\n",
       " '지금',\n",
       " '부터',\n",
       " '자연어',\n",
       " '처리',\n",
       " '수업',\n",
       " '을',\n",
       " '시작',\n",
       " '하',\n",
       " '겠',\n",
       " '습니다',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Komoran으로 토큰화\n",
    "from konlpy.tag import Komoran # 대소문자 구분 주의\n",
    "kmr = Komoran()\n",
    "kmr.morphs(kor_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3134c8",
   "metadata": {},
   "source": [
    "## 3. 품사 태깅 (POS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102fe09",
   "metadata": {},
   "source": [
    "### 3-1. NLTK 활용 품사 태깅 : nltk.pos_tag(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa01ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sent = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "\n",
    "# 토큰화\n",
    "word_tokens = word_tokenize(sent)\n",
    "print(word_tokens)\n",
    "\n",
    "# 품사태깅\n",
    "tagged = nltk.pos_tag(word_tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5904ca8",
   "metadata": {},
   "source": [
    "* 품사명 알아보기 : nltk.help.upenn_tagset(품사명)\n",
    "    - nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2713e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('tagsets_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bb94043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "# 품사명 확인\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe88966",
   "metadata": {},
   "source": [
    "### 3-2. 한글 형태소 분석과 품사 태깅 (konlpy.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e92f1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비추어줄까.\n",
    "정희성, 희망 공부'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7730d1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비추어줄까', '.', '정희성', ',', '희망', '공부']\n",
      "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비추어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# nltk로 한글 품사 태깅\n",
    "tokens = word_tokenize(kor_text)\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15aacb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 :  ['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '지금', '부터', '자연어', '처리', '수업', '을', '시작', '하겠습니다', '.']\n",
      "품사 태깅 :  [('안녕하세요', 'Adjective'), (',', 'Punctuation'), ('여러분', 'Noun'), ('.', 'Punctuation'), ('만나서', 'Verb'), ('반갑습니다', 'Adjective'), ('.', 'Punctuation'), ('지금', 'Noun'), ('부터', 'Josa'), ('자연어', 'Noun'), ('처리', 'Noun'), ('수업', 'Noun'), ('을', 'Josa'), ('시작', 'Noun'), ('하겠습니다', 'Verb'), ('.', 'Punctuation')]\n",
      "명사 추출 :  ['여러분', '지금', '자연어', '처리', '수업', '시작']\n"
     ]
    }
   ],
   "source": [
    "# Okt로 형태소 분석, 품사 태깅, 명사 추출\n",
    "print(\"형태소 분석 : \",okt.morphs(kor_text))\n",
    "print(\"품사 태깅 : \",okt.pos(kor_text))\n",
    "print(\"명사 추출 : \",okt.nouns(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a1cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 :  ['안녕하세요', ',', '여러분', '.', '만나', '아서', '반갑습니다', '.', '지금', '부터', '자연어', '처리', '수업', '을', '시작', '하', '겠', '습니다', '.']\n",
      "품사 태깅 :  [('안녕하세요', 'NNP'), (',', 'SP'), ('여러분', 'NNP'), ('.', 'SF'), ('만나', 'VV'), ('아서', 'EC'), ('반갑습니다', 'NNP'), ('.', 'SF'), ('지금', 'NNG'), ('부터', 'JX'), ('자연어', 'NNP'), ('처리', 'NNP'), ('수업', 'NNG'), ('을', 'JKO'), ('시작', 'NNG'), ('하', 'XSV'), ('겠', 'EP'), ('습니다', 'EF'), ('.', 'SF')]\n",
      "명사 추출 :  ['안녕하세요', '여러분', '반갑습니다', '지금', '자연어', '처리', '수업', '시작']\n"
     ]
    }
   ],
   "source": [
    "# Komoran으로 형태소 분석, 품사 태깅, 명사 추출\n",
    "print(\"형태소 분석 : \",kmr.morphs(kor_text))\n",
    "print(\"품사 태깅 : \",kmr.pos(kor_text))\n",
    "print(\"명사 추출 : \",kmr.nouns(kor_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc6e88",
   "metadata": {},
   "source": [
    "## 4. 정제 (Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78da4fa",
   "metadata": {},
   "source": [
    "### 4-1. 불필요한 기호 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cbb5f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone It's good to see you Let's start out NLP class \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, everyone. It's good to see you. Let's start out NLP class~~~~!\"\n",
    "\n",
    "# [영어] 문자, 숫자, ' 가 아닌 기호 삭제\n",
    "#꺽쇠안에 적힌 조건에 부합한 문자나 숫자를 2번째 인자에 적힌 것으로 변환\n",
    "# + = 패턴 1회 이상 반복\n",
    "clean_text = re.sub(\"[^A-Za-z0-9']+\", \" \", text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c77bd001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 여러분 만나서 반갑습니다 지금부터 자연어처리 수업을 시작하겠습니다 \n"
     ]
    }
   ],
   "source": [
    "# 한글만 남기고 삭제\n",
    "clean_kor_text = re.sub(\"[^가-힣]+\",\" \",kor_text)\n",
    "print(clean_kor_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e1e36",
   "metadata": {},
   "source": [
    "#### 정규 표현식 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbe66600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' 자연어처리~~~~ ', '^^']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text1 = \"543 cat camera 자연어처리~~~~ good^^\"\n",
    "\n",
    "#영어만, 숫자만, 한글만, 기호만\n",
    "p = re.compile('[^a-z0-9]+')\n",
    "print(p.findall(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1e492a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "이메일 주소를 입력하세요 :  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잘못된 이메일입니다.\n"
     ]
    }
   ],
   "source": [
    "# 이메일 주소 검증\n",
    "email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "email = input(\"이메일 주소를 입력하세요 : \")\n",
    "if email_pattern.match(email):\n",
    "    print(\"이메일 주소가 맞습니다.\")\n",
    "else:\n",
    "    print(\"잘못된 이메일입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36f37c",
   "metadata": {},
   "source": [
    "### 4-2. 불용어(Stopwords) 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c0f6a",
   "metadata": {},
   "source": [
    "#### 불용어 사전을 사용하여, 사전에 있는 단어를 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c4f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# [영어] nltk 불용어 사전 확인\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(len(english_stopwords))\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d25c39cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone it s good to see you let s start out nlp class \n",
      "['hello', 'everyone', 'good', 'see', 'let', 'start', 'nlp', 'class']\n"
     ]
    }
   ],
   "source": [
    "# [영어] 불용어 사전에 있는 단어 제거 : 정규화 -> 정제 -> 토큰화 -> 불용어 제거\n",
    "\n",
    "clean_text = re.sub(r\"[^\\w]+\", \" \", text.lower())\n",
    "print(clean_text)\n",
    "tokens = word_tokenize(clean_text)\n",
    "\n",
    "result = [word for word in tokens if word not in english_stopwords]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "679dcf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 :  ['의', '가', '은', '다', 'ㄴ', '에', '이', '듯', '은', '속', '에', '는', '에', '가', '하', '겠', 'ㅁ', '이', '적', '다면', '그', '가', '을', '어', '주', 'ㄹ까', '습니다', '.', ',']\n",
      "\n",
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '지금', '부터', '자연어', '처리', '수업', '을', '시작', '하겠습니다', '.']\n",
      "\n",
      "불용어 제거 전 :\n",
      " ['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '지금', '부터', '자연어', '처리', '수업', '을', '시작', '하겠습니다', '.']\n",
      "\n",
      "불용어 제거 후 :\n",
      " ['안녕하세요', '여러분', '만나서', '반갑습니다', '지금', '부터', '자연어', '처리', '수업', '시작', '하겠습니다']\n"
     ]
    }
   ],
   "source": [
    "# [한글] 사용자 정의 stopwords 를 만들어서 사용\n",
    "result = []\n",
    "temp = '의 가 은 다 ㄴ 에 이 듯 은 속 에 는 에 가 하 겠 ㅁ 이 적 다면 그 가 을 어 주 ㄹ까 습니다 . ,'\n",
    "kor_stopwords = temp.split()\n",
    "print(\"불용어 : \", kor_stopwords)\n",
    "print()\n",
    "\n",
    "# 토큰화\n",
    "\n",
    "tokens = okt.morphs(kor_text)\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "# 불용어 제거\n",
    "# stopword에 속하지 않는 문자를 붙이기\n",
    "# 아래 for문 3줄의 경우 list comprehension 방식을 사용한 1줄 코딩이 가능하다 \n",
    "# result = [word for word in tokens if word not in kor_stopwords]\n",
    "for word in tokens:\n",
    "    if word not in kor_stopwords:\n",
    "        result.append(word)\n",
    "\n",
    "print('불용어 제거 전 :\\n',tokens) \n",
    "print()\n",
    "print('불용어 제거 후 :\\n',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a813c",
   "metadata": {},
   "source": [
    "#### 품사 태깅을 하여 불용어에 해당하는 품사의 단어를 삭제 \n",
    "* 한국어 형태소 해석기의 품사태그집합 정보\n",
    "https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit?gid=0#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac006a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅 결과 검토 -> 원하는 품사 추출 \n",
    "my_tag_set = ['NN', 'VB', 'NNP']\n",
    "my_words = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7842deb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n",
      "['Hello', 'everyone', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "# 원하는 품사의 단어들만 추출\n",
    "pos_tag_tokens = nltk.pos_tag(word_tokens)\n",
    "print(pos_tag_tokens)\n",
    "for word, tag in pos_tag_tokens:\n",
    "    if tag in my_tag_set:\n",
    "        my_words.append(word)\n",
    "print(my_words)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ada2b",
   "metadata": {},
   "source": [
    "## 5. 정규화 (Normalization)\n",
    "### 5-1. 어간 추출 (Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c59e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "print(p_stemmer.stem('cooking'), p_stemmer.stem('cookery'), p_stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2b454d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "l_stemmer = LancasterStemmer()\n",
    "print(l_stemmer.stem('cooking'), l_stemmer.stem('cookery'), l_stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04957c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2535546570.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    p_result =\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "p_result =  \n",
    "l_result = \n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:', p_result)\n",
    "print('랭커스터 스테머의 어간 추출 후:', l_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e971d",
   "metadata": {},
   "source": [
    "### 5-2 표제어 추출 (Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b71f7b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v')) #품사를 지정\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39469def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming result: believ\n",
      "lemmatizing result: belief\n",
      "lemmatizing result: believe\n"
     ]
    }
   ],
   "source": [
    "#comparison of lemmatizing and stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print('stemming result:', stemmer.stem('believes'))\n",
    "print('lemmatizing result:', lemmatizer.lemmatize('believes'))\n",
    "print('lemmatizing result:', lemmatizer.lemmatize('believes', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e26d987c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비추어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
      "\n",
      "['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나다', '\\n', '희망', '은', '절망', '속', '에', '싹트다', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다', '\\n', '그', '누가', '세상', '을', '비추다', '.', '\\n', '정희성', ',', '희망', '공부']\n"
     ]
    }
   ],
   "source": [
    "# Okt로 형태소 분석 시 표제어 추출\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs(kor_text))\n",
    "print()\n",
    "print(okt.morphs(kor_text, stem = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepNLPEnv",
   "language": "python",
   "name": "deepnlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
